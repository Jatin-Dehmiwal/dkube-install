{
  all(params):: [
    $.parts(params.namespace).fluentdservice(),
    $.parts(params.namespace).fluentdLoggerConfigmap(),
    $.parts(params.namespace).fluentdMetricCollectorConfigmap(),
  ],

  parts(namespace):: {
    fluentdservice():: {
        "apiVersion": "v1",
        "kind": "Service",
        "metadata": {
            "annotations": {
                "prometheus.io/port": "24231",
                "prometheus.io/scrape": "true"
            },
            "labels": {
                "app": "dkube-log-processor"
            },
            "name": "dkube-log-processor",
            "namespace": "dkube"
        },
        "spec": {
            "ports": [
                {
                    "name": "dkube-log-metrics",
                    "port": 24231,
                    "protocol": "TCP",
                    "targetPort": 24231
                }
            ],
            "selector": {
                "k8s-app": "dkube-metric-collector"
            },
            "type": "ClusterIP"
        }
    },
    fluentdLoggerConfigmap():: {
        "apiVersion": "v1",
        "data": {
            "fluent.conf": "<source>\r\n  @type tail\r\n  path \/var\/log\/containers\/*.log\r\n  pos_file \/var\/log\/fluentd-containers-jobs.log.pos\r\n  time_key time\r\n  time_format %Y-%m-%dT%H:%M:%S\r\n  refresh_interval 1s\r\n  open_on_every_update true\r\n  enable_stat_watcher false\r\n  rotate_wait 2\r\n  tag kubernetes_jobs.*\r\n  @label @JOBS\r\n  format json\r\n  read_from_head true\r\n<\/source>\r\n\r\n<source>\r\n  @type tail\r\n  path \/var\/log\/containers\/*.log\r\n  pos_file \/var\/log\/fluentd-containers-pl.log.pos\r\n  time_key time\r\n  time_format %Y-%m-%dT%H:%M:%S\r\n  refresh_interval 1s\r\n  open_on_every_update true\r\n  enable_stat_watcher false\r\n  rotate_wait 2\r\n  tag kubernetes_pl.*\r\n  @label @PIPELINE\r\n  format json\r\n  read_from_head true\r\n<\/source>\r\n\r\n<source>\r\n  @type tail\r\n  path \/var\/log\/containers\/*_dkube-d3api-*.log\r\n  pos_file \/var\/log\/fluentd-containers-api.log.pos\r\n  time_key time\r\n  time_format %Y-%m-%dT%H:%M:%S\r\n  refresh_interval 1s\r\n  open_on_every_update true\r\n  enable_stat_watcher false\r\n  rotate_wait 2\r\n  tag kubernetes_apis.*\r\n  @label @MASTER\r\n  format json\r\n  read_from_head true\r\n<\/source>\r\n\r\n\r\n<label @JOBS>\r\n    #adding kubernetes metadata for accessing labels\r\n     <filter kubernetes_jobs.**>\r\n        @type kubernetes_metadata\r\n    <\/filter>\r\n\r\n     #collect logs of pod which has label(logger: dkube)\r\n    <filter kubernetes_jobs.**>\r\n        @type grep\r\n        <regexp>\r\n            key $.kubernetes.labels.logger\r\n            pattern \/^dkube$\/\r\n        <\/regexp>\r\n    <\/filter>\r\n\r\n    <filter kubernetes_jobs.**>\r\n       @type record_modifier\r\n       enable_ruby\r\n       <record>\r\n           jobname ${record.dig(\"kubernetes\", \"labels\", \"jobname\")}\r\n           username ${record.dig(\"kubernetes\", \"labels\", \"username\")}\r\n           container ${record.dig(\"kubernetes\", \"container_name\")}\r\n           message ${record.dig(\"kubernetes\", \"labels\", \"tf-replica-type\")}-${record.dig(\"kubernetes\", \"labels\", \"tf-replica-index\")}:  ${record.dig(\"log\")}\r\n           messagefile tfjob ${record.dig(\"kubernetes\", \"labels\", \"tf-replica-type\")}-${record.dig(\"kubernetes\", \"labels\", \"tf-replica-index\")}: ${record.dig(\"log\")}\r\n       <\/record>\r\n       remove_keys log, stream, docker, kubernetes\r\n    <\/filter>\r\n\r\n    <match kubernetes_jobs.**>\r\n      @type copy\r\n      #ouput to file\r\n      <store>\r\n        @type file\r\n        path \/var\/log\/dkube\/tfjob\r\n        append true\r\n        <format>\r\n          @type single_value\r\n          message_key messagefile\r\n          add_newline false\r\n        <\/format>\r\n        <buffer time>\r\n          @type file\r\n          timekey 10s\r\n          timekey_wait 10s\r\n          timekey_use_utc true\r\n          chunk_limit_size 256m\r\n          flush_thread_count 8\r\n          retry_forever true\r\n          #retry_max_times 100\r\n          overflow_action throw_exception\r\n          retry_type exponential_backoff\r\n          retry_exponential_backoff_base 2\r\n          flush_mode immediate\r\n          flush_at_shutdown true\r\n          queue_limit_length 10000\r\n        <\/buffer>\r\n      <\/store>\r\n      #ouput to s3\r\n      <store>\r\n        @type s3\r\n        aws_key_id dkube\r\n        aws_sec_key l06dands19s\r\n        s3_endpoint http:\/\/dkube-minio-server.dkube:9000\/\r\n        s3_bucket dkube\r\n        path system\/logs\/${username}\/${jobname}\/${container}\r\n        s3_object_key_format %{path}\/job-log-%{index}.%{file_extension}\r\n        store_as text\r\n        force_path_style true\r\n        <format>\r\n          @type single_value\r\n          message_key message\r\n          add_newline false\r\n        <\/format>\r\n        <buffer username, jobname, container>\r\n          @type file\r\n          path \/var\/log\/td-agent\/jobs\/${username}\/${jobname}\/${container}\r\n          timekey 10s            # Flush the accumulated chunks every hour\r\n          timekey_wait 10s        # Wait for 60 seconds before flushing\r\n          timekey_use_utc true   # Use this option if you prefer UTC timestamps\r\n          chunk_limit_size 256m  # The maximum size of each chunk\r\n          flush_thread_count 8\r\n          retry_forever true\r\n          #retry_max_times 100\r\n          overflow_action throw_exception\r\n          retry_type exponential_backoff\r\n          retry_exponential_backoff_base 2\r\n          flush_mode immediate\r\n          flush_at_shutdown true\r\n          queue_limit_length 10000\r\n        <\/buffer>\r\n      <\/store>\r\n    <\/match>\r\n<\/label>\r\n\r\n\r\n<label @PIPELINE>\r\n    #adding kubernetes metadata for accessing labels\r\n     <filter kubernetes_pl.**>\r\n        @type kubernetes_metadata\r\n    <\/filter>\r\n\r\n     #collect logs of pod which has label(logger: dkube)\r\n    <filter kubernetes_pl.**>\r\n        @type grep\r\n        <regexp>\r\n            key $.kubernetes.labels.logger\r\n            pattern \/^dkubepl$\/\r\n        <\/regexp>\r\n    <\/filter>\r\n\r\n    # framing new records\r\n    <filter kubernetes_pl.**>\r\n       @type record_modifier\r\n       enable_ruby\r\n       <record>\r\n           podname ${record.dig(\"kubernetes\", \"labels\", \"runid\")}\r\n           wfname ${record.dig(\"kubernetes\", \"labels\", \"workflows_argoproj_io\/workflow\")}\r\n           container ${record.dig(\"kubernetes\", \"container_name\")}\r\n           messagefile pipeline ${record.dig(\"kubernetes\", \"labels\", \"workflows_argoproj_io\/workflow\")}-${record.dig(\"kubernetes\", \"labels\", \"runid\")}-${record.dig(\"log\")}\r\n       <\/record>\r\n       remove_keys  stream, docker, kubernetes\r\n    <\/filter>\r\n\r\n    <match kubernetes_pl.**>\r\n      @type copy\r\n      #ouput to file\r\n      <store>\r\n        @type file\r\n        path \/var\/log\/dkube\/pipeline\r\n        append true\r\n        <format>\r\n          @type single_value\r\n          message_key messagefile\r\n          add_newline false\r\n        <\/format>\r\n        <buffer time>\r\n          @type file\r\n          timekey 10s            # Flush the accumulated chunks every hour\r\n          timekey_wait 10s        # Wait for 60 seconds before flushing\r\n          timekey_use_utc true   # Use this option if you prefer UTC timestamps\r\n          chunk_limit_size 256m  # The maximum size of each chunk\r\n          flush_thread_count 8\r\n          retry_forever true\r\n          overflow_action throw_exception\r\n          retry_type exponential_backoff\r\n          retry_exponential_backoff_base 2\r\n          retry_wait 5s\r\n          flush_mode immediate\r\n          flush_at_shutdown true\r\n          queue_limit_length 10000\r\n        <\/buffer>\r\n      <\/store>\r\n      #ouput to s3\r\n      <store>\r\n        @type s3\r\n        aws_key_id dkube\r\n        aws_sec_key l06dands19s\r\n        s3_endpoint http:\/\/dkube-minio-server.dkube:9000\/\r\n        s3_bucket dkube\r\n        path system\/logs\/kubeflow\/${wfname}\/${podname}\/${container}\r\n        s3_object_key_format %{path}\/pl-log-%{index}.%{file_extension}\r\n        store_as text\r\n        force_path_style true\r\n        <format>\r\n          @type single_value\r\n          message_key log\r\n          add_newline false\r\n        <\/format>\r\n        <buffer wfname, podname, container>\r\n          @type file\r\n          path \/var\/log\/td-agent\/pljobs\/${wfname}\/${podname}\/${container}\r\n          timekey 10s            # Flush the accumulated chunks every hour\r\n          timekey_wait 10s        # Wait for 60 seconds before flushing\r\n          timekey_use_utc true   # Use this option if you prefer UTC timestamps\r\n          chunk_limit_size 256m  # The maximum size of each chunk\r\n          flush_thread_count 8\r\n          retry_forever true\r\n          overflow_action throw_exception\r\n          retry_type exponential_backoff\r\n          retry_exponential_backoff_base 2\r\n          retry_wait 5s\r\n          flush_mode immediate\r\n          flush_at_shutdown true\r\n          queue_limit_length 10000\r\n        <\/buffer>\r\n      <\/store>\r\n    <\/match>\r\n<\/label>\r\n\r\n<label @MASTER>\r\n    #adding kubernetes metadata for accessing labels\r\n     <filter kubernetes_apis.**>\r\n        @type kubernetes_metadata\r\n    <\/filter>\r\n\r\n     #collect logs of pod which has label(logger: dkube)\r\n    <filter kubernetes_apis.**>\r\n        @type grep\r\n        <regexp>\r\n            key $.kubernetes.labels.app\r\n            pattern dkube-controller-master\r\n        <\/regexp>\r\n    <\/filter>\r\n\r\n    <filter kubernetes_apis.**>\r\n       @type record_modifier\r\n       enable_ruby\r\n       <record>\r\n           message d3system ${record.dig(\"log\")}\r\n       <\/record>\r\n    <\/filter>\r\n\r\n    <match kubernetes_apis.**>\r\n      @type copy\r\n      #ouput to file\r\n      <store>\r\n        @type file\r\n        path \/var\/log\/dkube\/controller\r\n        append true\r\n        <format>\r\n          @type single_value\r\n          message_key message\r\n          add_newline false\r\n        <\/format>\r\n        <buffer time>\r\n          @type file\r\n          timekey 10s\r\n          timekey_wait 10s\r\n          timekey_use_utc true\r\n          chunk_limit_size 256m\r\n          flush_thread_count 8\r\n          retry_forever true\r\n          overflow_action throw_exception\r\n          retry_type exponential_backoff\r\n          retry_exponential_backoff_base 2\r\n          retry_wait 5s\r\n          flush_mode immediate\r\n          flush_at_shutdown true\r\n          queue_limit_length 10000\r\n        <\/buffer>\r\n      <\/store>\r\n    <\/match>\r\n<\/label>\r\n"
        },
        "kind": "ConfigMap",
        "metadata": {
            "name": "dkube-log-collector",
            "namespace": "dkube"
        }
    },
    fluentdMetricCollectorConfigmap():: {
        "apiVersion": "v1",
        "data": {
            "accuracy.conf": "\u003cfilter kubernetes_accuracy.**\u003e\n    @type kubernetes_metadata\n\u003c/filter\u003e\n\n\u003cfilter kubernetes_accuracy.**\u003e\n    @type grep\n    \u003cregexp\u003e\n        key $.kubernetes.labels.logger\n        pattern /^dkube$/\n    \u003c/regexp\u003e\n\u003c/filter\u003e\n\n\u003cfilter kubernetes_accuracy.**\u003e\n    @type grep\n    \u003cregexp\u003e\n        key log\n        pattern /accuracy/\n    \u003c/regexp\u003e\n\u003c/filter\u003e\n\n\u003cfilter kubernetes_accuracy.**\u003e\n  @type parser\n  key_name $.log\n  reserve_data true\n  remove_key_name_field true\n  suppress_parse_error_log true\n  \u003cparse\u003e\n    @type regexp\n    expression /^(.*):(.*):((.*)])?((.*):)?(?\u003cmessage\u003e(.*))$/\n  \u003c/parse\u003e\n\u003c/filter\u003e\n\n \u003cfilter kubernetes_accuracy.**\u003e\n   @type record_modifier\n   enable_ruby\n   \u003crecord\u003e\n       escaped_tag ${record[\"message\"].gsub(' ', '')}\n   \u003c/record\u003e\n \u003c/filter\u003e\n\n\u003cfilter kubernetes_accuracy.**\u003e\n    @type grep\n    \u003cregexp\u003e\n        key escaped_tag\n        pattern /accuracy=/\n    \u003c/regexp\u003e\n\u003c/filter\u003e\n\n\u003cfilter kubernetes_accuracy.**\u003e\n  @type parser\n  key_name $.escaped_tag\n  reserve_data true\n  remove_key_name_field true\n  suppress_parse_error_log true\n  \u003cparse\u003e\n    @type ltsv\n    delimiter_pattern /,/\n    label_delimiter  =\n  \u003c/parse\u003e\n\u003c/filter\u003e\n\n\u003cfilter kubernetes_accuracy.**\u003e\n   @type record_modifier\n   enable_ruby\n   \u003crecord\u003e\n       jobname ${record.dig(\"kubernetes\", \"labels\", \"jobname\")}\n       username ${record.dig(\"kubernetes\", \"labels\", \"username\")}\n       jobid ${record.dig(\"kubernetes\", \"labels\", \"jobid\")}\n       mode ${record.dig(\"mode\").to_s}\n       step ${record.dig(\"step\").to_i}\n       epoch ${record.dig(\"epoch\").to_i}\n       accuracy ${record.dig(\"accuracy\").to_f} \n   \u003c/record\u003e\n\u003c/filter\u003e\n\n\u003cfilter kubernetes_accuracy.**\u003e\n  @type prometheus\n  \u003cmetric\u003e\n    name accuracy\n    type gauge\n    desc accuracy metric\n    key $.accuracy\n    \u003clabels\u003e\n      jobname ${jobname}\n      username ${username}\n      jobid ${jobid}\n      step ${step}\n      mode ${mode}\n      epoch ${epoch}\n    \u003c/labels\u003e\n  \u003c/metric\u003e\n\u003c/filter\u003e\n\n\u003cmatch kubernetes_accuracy.**\u003e\n    @type relabel\n    @label @PROMETHEUS\n\u003c/match\u003e\n",
            "fluent.conf": "\u003csource\u003e\n  @type prometheus\n\u003c/source\u003e\n\n\u003csource\u003e\n  @type monitor_agent\n\u003c/source\u003e\n\n\u003csource\u003e\n  @type forward\n\u003c/source\u003e\n\n# input plugin that collects metrics from MonitorAgent\n\u003csource\u003e\n  @type prometheus_monitor\n  \u003clabels\u003e\n    host ${hostname}\n  \u003c/labels\u003e\n\u003c/source\u003e\n\n# input plugin that collects metrics for output plugin\n\u003csource\u003e\n  @type prometheus_output_monitor\n  \u003clabels\u003e\n    host ${hostname}\n  \u003c/labels\u003e\n\u003c/source\u003e\n\n# input plugin that collects metrics for in_tail plugin\n\u003csource\u003e\n  @type prometheus_tail_monitor\n  \u003clabels\u003e\n    host ${hostname}\n  \u003c/labels\u003e\n\u003c/source\u003e\n\n\u003csource\u003e\n  @type tail\n  path /var/log/containers/*_tensorflow-*.log\n  pos_file /var/log/fluentd-containers-accuracy.log.pos\n  time_format %Y-%m-%dT%H:%M:%S\n  tag kubernetes_accuracy.*\n  @label @ACCURACY\n  format json\n  read_from_head true\n\u003c/source\u003e\n\n\u003csource\u003e\n  @type tail\n  path /var/log/containers/*_tensorflow-*.log\n  pos_file /var/log/fluentd-containers-loss.log.pos\n  time_format %Y-%m-%dT%H:%M:%S\n  tag kubernetes_loss.*\n  @label @LOSS\n  format json\n  read_from_head true\n\u003c/source\u003e\n\n\u003csource\u003e\n  @type tail\n  path /var/log/containers/*_tensorflow-*.log\n  pos_file /var/log/fluentd-containers-step.log.pos\n  time_format %Y-%m-%dT%H:%M:%S\n  tag kubernetes_step.*\n  @label @STEP\n  format json\n  read_from_head true\n\u003c/source\u003e\n\n\u003clabel @STEP\u003e\n    @include step.conf\n\u003c/label\u003e\n\n\u003clabel @ACCURACY\u003e\n    @include accuracy.conf\n\u003c/label\u003e\n\n\u003clabel @LOSS\u003e\n    @include loss.conf\n\u003c/label\u003e\n\n\u003clabel @PROMETHEUS\u003e\n    @include prometheus.conf\n\u003c/label\u003e\n",
            "loss.conf": "\u003cfilter kubernetes_loss.**\u003e\n     @type kubernetes_metadata\n \u003c/filter\u003e\n\n \u003cfilter kubernetes_loss.**\u003e\n     @type grep\n     \u003cregexp\u003e\n         key $.kubernetes.labels.logger\n         pattern /^dkube$/\n     \u003c/regexp\u003e\n \u003c/filter\u003e\n\n \u003cfilter kubernetes_loss.**\u003e\n     @type grep\n     \u003cregexp\u003e\n         key log\n         pattern /loss/\n     \u003c/regexp\u003e\n \u003c/filter\u003e\n\n \u003cfilter kubernetes_loss.**\u003e\n   @type parser\n   key_name $.log\n   reserve_data true\n   remove_key_name_field true\n   suppress_parse_error_log true\n   \u003cparse\u003e\n     @type regexp\n     expression /^(.*):(.*):((.*)])?((.*):)?(?\u003cmessage\u003e(.*))$/\n   \u003c/parse\u003e\n \u003c/filter\u003e\n\n  \u003cfilter kubernetes_loss.**\u003e\n    @type record_modifier\n    enable_ruby\n    \u003crecord\u003e\n        escaped_tag ${record[\"message\"].gsub(' ', '')}\n    \u003c/record\u003e\n  \u003c/filter\u003e\n\n \u003cfilter kubernetes_loss.**\u003e\n     @type grep\n     \u003cregexp\u003e\n         key escaped_tag\n         pattern /loss=/\n     \u003c/regexp\u003e\n \u003c/filter\u003e\n\n \u003cfilter kubernetes_loss.**\u003e\n   @type parser\n   key_name $.escaped_tag\n   reserve_data true\n   remove_key_name_field true\n   suppress_parse_error_log true\n   \u003cparse\u003e\n     @type ltsv\n     delimiter_pattern /,/\n     label_delimiter  =\n   \u003c/parse\u003e\n \u003c/filter\u003e\n\n \u003cfilter kubernetes_loss.**\u003e\n    @type record_modifier\n    enable_ruby\n    \u003crecord\u003e\n        jobname ${record.dig(\"kubernetes\", \"labels\", \"jobname\")}\n        username ${record.dig(\"kubernetes\", \"labels\", \"username\")}\n        jobuuid ${record.dig(\"kubernetes\", \"labels\", \"jobuuid\")}\n        jobid ${record.dig(\"kubernetes\", \"labels\", \"jobid\")}\n        step ${record.dig(\"step\").to_i}\n        loss ${record.dig(\"loss\").to_f}\n        mode ${record.dig(\"mode\").to_s}\n        epoch ${record.dig(\"epoch\").to_i} \n    \u003c/record\u003e\n \u003c/filter\u003e\n\n \u003cfilter kubernetes_loss.**\u003e\n   @type prometheus\n   \u003cmetric\u003e\n     name loss\n     type gauge\n     desc loss metric\n     key $.loss\n     \u003clabels\u003e\n       jobuuid ${jobuuid}\n       jobname ${jobname}\n       username ${username}\n       jobid ${jobid}\n       step ${step}\n       mode ${mode}\n       epoch ${epoch}\n     \u003c/labels\u003e\n   \u003c/metric\u003e\n \u003c/filter\u003e\n\n \u003cmatch kubernetes_loss.**\u003e\n     @type relabel\n     @label @PROMETHEUS\n \u003c/match\u003e\n",
            "prometheus.conf": "\u003cmatch {kubernetes_accuracy.**, kubernetes_loss.**, kubernetes_step.**}\u003e\n  @type copy\n  # for MonitorAgent sample\n  \u003cstore\u003e\n    @id test_forward\n    @type forward\n    buffer_type memory\n    flush_interval 1s\n    max_retry_wait 2s\n    send_timeout 60s\n    recover_wait 60s\n    hard_timeout 60s\n    \u003cbuffer\u003e\n      # max_retry_wait 10s\n      flush_interval 1s\n      # retry_type periodic\n      disable_retry_limit\n   \u003c/buffer\u003e\n   # retry_limit 3\n   disable_retry_limit\n   \u003cserver\u003e\n     host 0.0.0.0\n     port 24224\n   \u003c/server\u003e\n  \u003c/store\u003e\n\u003c/match\u003e\n",
            "step.conf": "\u003cfilter kubernetes_step.**\u003e\n    @type kubernetes_metadata\n\u003c/filter\u003e\n\n\u003cfilter kubernetes_step.**\u003e\n    @type grep\n    \u003cregexp\u003e\n        key $.kubernetes.labels.logger\n        pattern /^dkube$/\n    \u003c/regexp\u003e\n\u003c/filter\u003e\n\n\u003cfilter kubernetes_step.**\u003e\n    @type grep\n    \u003cregexp\u003e\n        key log\n        pattern /step/\n    \u003c/regexp\u003e\n\u003c/filter\u003e\n\n\u003cfilter kubernetes_step.**\u003e\n  @type parser\n  key_name $.log\n  reserve_data true\n  remove_key_name_field true\n  suppress_parse_error_log true\n  \u003cparse\u003e\n    @type regexp\n    expression /^(.*):(.*):((.*)])?((.*):)?(?\u003cmessage\u003e(.*))$/\n  \u003c/parse\u003e\n\u003c/filter\u003e\n\n \u003cfilter kubernetes_step.**\u003e\n   @type record_modifier\n   enable_ruby\n   \u003crecord\u003e\n       escaped_tag ${record[\"message\"].gsub(' ', '')}\n   \u003c/record\u003e\n \u003c/filter\u003e\n\n\u003cfilter kubernetes_step.**\u003e\n    @type grep\n    \u003cregexp\u003e\n        key escaped_tag\n        pattern /,step=/\n    \u003c/regexp\u003e\n\u003c/filter\u003e\n\n\u003cfilter kubernetes_step.**\u003e\n  @type parser\n  key_name $.escaped_tag\n  reserve_data true\n  remove_key_name_field true\n  suppress_parse_error_log true\n  \u003cparse\u003e\n    @type ltsv\n    delimiter_pattern /,/\n    label_delimiter  =\n  \u003c/parse\u003e\n\u003c/filter\u003e\n\n\u003cfilter kubernetes_step.**\u003e\n   @type record_modifier\n   enable_ruby\n   \u003crecord\u003e\n       jobname ${record.dig(\"kubernetes\", \"labels\", \"jobname\")}\n       username ${record.dig(\"kubernetes\", \"labels\", \"username\")}\n       jobid ${record.dig(\"kubernetes\", \"labels\", \"jobid\")}\n       step ${record.dig(\"step\").to_i}\n       accuracy ${record.dig(\"accuracy\").to_f}\n       loss ${record.dig(\"loss\").to_f}\n       mode ${record.dig(\"mode\").to_s}\n   \u003c/record\u003e\n\u003c/filter\u003e\n\n\u003cfilter kubernetes_step.**\u003e\n  @type prometheus\n  \u003cmetric\u003e\n    name step\n    type gauge\n    desc step metric\n    key $.step\n    \u003clabels\u003e\n      jobname ${jobname}\n      username ${username}\n      jobid ${jobid}\n      accuracy ${accuracy}\n      loss ${loss}\n      mode ${mode}\n    \u003c/labels\u003e\n  \u003c/metric\u003e\n\u003c/filter\u003e\n\n\u003cmatch kubernetes_step.**\u003e\n    @type relabel\n    @label @PROMETHEUS\n\u003c/match\u003e\n"
        },
        "kind": "ConfigMap",
        "metadata": {
            "name": "dkube-metric-collector",
            "namespace": "dkube"
        }
     }
  },
}
